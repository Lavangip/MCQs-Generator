{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import string"
      ],
      "metadata": {
        "id": "3quVOGfjz22f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp= spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "5Nfa403QkzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = nltk.FreqDist(brown.words())"
      ],
      "metadata": {
        "id": "ORx8IhULlpsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_common(word):\n",
        "    return freq_dist[word.lower()] > 1000 or word.lower() in string.punctuation\n",
        "\n",
        "def is_significant(word):\n",
        "    return len(word) > 3"
      ],
      "metadata": {
        "id": "hH9RzoMovX95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mcqs(text, num_ques=5):\n",
        "    if text is None or text.strip() == \"\":\n",
        "        return []\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    selected_sentences = random.sample(sentences, min(num_ques, len(sentences)))\n",
        "    mcqs = []\n",
        "\n",
        "    for sentence in selected_sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sent_doc = nlp(sentence)\n",
        "        nouns = [token.text for token in sent_doc if token.pos_ == 'NOUN' and is_significant(token.text) and not is_common(token.text)]\n",
        "\n",
        "        if len(nouns) < 2:\n",
        "            continue\n",
        "\n",
        "        noun_counts = Counter(nouns)\n",
        "        if noun_counts:\n",
        "            subject = sorted(noun_counts.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
        "            answer_choices = [subject]\n",
        "\n",
        "            # Replace subject with blank\n",
        "            ques = sentence.replace(subject, \"______\")\n",
        "\n",
        "            # Collect distractors\n",
        "            distractors = list(set(nouns) - {subject})\n",
        "            if len(distractors) < 3:\n",
        "                distractors = distractors + [\"[Distractor]\"] * (3 - len(distractors))\n",
        "\n",
        "            # Ensure we have 3 distractors\n",
        "            random.shuffle(distractors)\n",
        "            answer_choices.extend(distractors[:3])\n",
        "            random.shuffle(answer_choices)\n",
        "\n",
        "            correct_ans = chr(65 + answer_choices.index(subject))\n",
        "            mcqs.append((ques, answer_choices, correct_ans))\n",
        "\n",
        "    return mcqs"
      ],
      "metadata": {
        "id": "I4FyAO1jnu0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}